{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정규표현식\n",
    "\n",
    "개체명 인식(named entity recognition)을 위해서 가장 먼저 활용할 수 있고 단순한 방법이 **정규표현식(regular expression)** 을 사용하는 것이다.\n",
    "예를 들어 대문자로 시작되는 단어를 개체명(entity)으로 인식하는 사례를 가장 처음 떠올릴 수가 있다.\n",
    "정규표현식으로 대문자 첫자로 시작되는 단어를 개체명으로 잡아낼 수 있도록 작성하게 되면 문장에서 이러한 패턴을 따르는 것을 뽑아낼 순서대로 뽑아낼 수가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['John', 'Atlanta']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "entity_pattern = re.compile(\"[A-Z]{1}[a-zA-Z]*\")\n",
    "\n",
    "another_sentence = \"John is from Atlanta\"\n",
    "entity_pattern.findall(another_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 개체명 인식의 정규표현식\n",
    "\n",
    "상기와 같이 개체명인식이 나름 성공적인 사례도 있지만, 다음과 같이 챗봇을 개발할 때 하나의 사례로 인사를 하는 것을 살펴보자. 즉, 챗봇이 \"인사\"라는 의도를 잡아내는데 **정규표현식** 을 적용한 사례로 ... 결론은 \"인사\" 의도(intent)를 잡아낼 수는 있으나 망가지기 쉬워 일일이 코딩이 필요한 사례로 정규표현식의 가능성과 한계를 명확히 보여주고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'안녕'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def identify_greeting(string):\n",
    "    \"\"\" 인사 패턴과 매칭되면 인식된 인사를 반환.\n",
    "        예를 들어, 안녕 등\"\"\"\n",
    "    if string[:2] == '안녕':\n",
    "        if string[:2] in ['안녕', '안녕하세요 ', '안녕ㅎ', '안녕!']:\n",
    "            return string[:2]\n",
    "        elif string[:6] in ['Hello', 'Hello ', 'Hello,', 'Hello!']:\n",
    "            return string[:5]\n",
    "    elif string[0] == '방':\n",
    "        if string[:2] in ['방가', '방가방가 ', '방가워요', '방갑습니다']:\n",
    "            return string\n",
    "    return None\n",
    "\n",
    "identify_greeting('안녕하세요.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'방가워요'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identify_greeting('방가워요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(identify_greeting('만나서 반갑습니다.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`identify_greeting()` 함수는 문자열에 안녕 혹은 방가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK 개체명 인식\n",
    "\n",
    "NLTK 라이브러리를 사용한 개체명 인식(Named Entity Recognition, NER) 방법을 살펴보자\n",
    "\n",
    "- [NLTK를 이용한 개체명 인식(Named Entity Recognition using NTLK)](https://wikidocs.net/30682)\n",
    "- [Susan Li (Aug 17, 2018), \"Named Entity Recognition with NLTK and SpaCy - NER is used in many fields in Natural Language Processing (NLP)\"](https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'JJ'), ('authorities', 'NNS'), ('fined', 'VBD'), ('Google', 'NNP'), ('a', 'DT'), ('record', 'NN'), ('$', '$'), ('5.1', 'CD'), ('billion', 'CD'), ('on', 'IN'), ('Wednesday', 'NNP'), ('for', 'IN'), ('abusing', 'VBG'), ('its', 'PRP$'), ('power', 'NN'), ('in', 'IN'), ('the', 'DT'), ('mobile', 'JJ'), ('phone', 'NN'), ('market', 'NN'), ('and', 'CC'), ('ordered', 'VBD'), ('the', 'DT'), ('company', 'NN'), ('to', 'TO'), ('alter', 'VB'), ('its', 'PRP$'), ('practices', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "sentence = \"European authorities fined Google a record $5.1 billion on Wednesday for abusing its power \\\n",
    "            in the mobile phone market and ordered the company to alter its practices\"\n",
    "sentence_pos = pos_tag(word_tokenize(sentence))\n",
    "print(sentence_pos) # 토큰화와 품사 태깅을 동시 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IOB 태그는 파일은 파일의 말뭉치 덩어리(chunk) 구조를 표현하는 표준으로 자리잡고 있다. 이를 활용하여 표현하면 문장을 표현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  European/JJ\n",
      "  authorities/NNS\n",
      "  fined/VBD\n",
      "  Google/NNP\n",
      "  (NP a/DT record/NN)\n",
      "  $/$\n",
      "  5.1/CD\n",
      "  billion/CD\n",
      "  on/IN\n",
      "  Wednesday/NNP\n",
      "  for/IN\n",
      "  abusing/VBG\n",
      "  its/PRP$\n",
      "  (NP power/NN)\n",
      "  in/IN\n",
      "  (NP the/DT mobile/JJ phone/NN)\n",
      "  (NP market/NN)\n",
      "  and/CC\n",
      "  ordered/VBD\n",
      "  (NP the/DT company/NN)\n",
      "  to/TO\n",
      "  alter/VB\n",
      "  its/PRP$\n",
      "  practices/NNS)\n"
     ]
    }
   ],
   "source": [
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sentence_pos)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nltk.chunk.conlltags2tree()` 함수는 태그 시퀀스를 말뭉치 나무구조로 변환시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'JJ', 'O'),\n",
      " ('authorities', 'NNS', 'O'),\n",
      " ('fined', 'VBD', 'O'),\n",
      " ('Google', 'NNP', 'O'),\n",
      " ('a', 'DT', 'B-NP'),\n",
      " ('record', 'NN', 'I-NP'),\n",
      " ('$', '$', 'O'),\n",
      " ('5.1', 'CD', 'O'),\n",
      " ('billion', 'CD', 'O'),\n",
      " ('on', 'IN', 'O'),\n",
      " ('Wednesday', 'NNP', 'O'),\n",
      " ('for', 'IN', 'O'),\n",
      " ('abusing', 'VBG', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('power', 'NN', 'B-NP'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('mobile', 'JJ', 'I-NP'),\n",
      " ('phone', 'NN', 'I-NP'),\n",
      " ('market', 'NN', 'B-NP'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('ordered', 'VBD', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('company', 'NN', 'I-NP'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('alter', 'VB', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('practices', 'NNS', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IOB 형태로 개체명 추출\n",
    "\n",
    "입력받은 문장을 NLTK 라이브러리를 활용하여 개체명을 추출할 경우 `Tree` 객체로 구성되어 있어 이를 IOB 형태로 변환시키려면 `tree2conlltags()` 함수를 사용해서 변환을 시킨다.\n",
    "\n",
    "- [Complete guide to build your own Named Entity Recognizer with Python](https://nlpforhackers.io/named-entity-extraction/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('European', 'JJ', 'O'), ('authorities', 'NNS', 'O'), ('fined', 'VBD', 'O'), ('Google', 'NNP', 'O'), ('a', 'DT', 'B-NP'), ('record', 'NN', 'I-NP'), ('$', '$', 'O'), ('5.1', 'CD', 'O'), ('billion', 'CD', 'O'), ('on', 'IN', 'O'), ('Wednesday', 'NNP', 'O'), ('for', 'IN', 'O'), ('abusing', 'VBG', 'O'), ('its', 'PRP$', 'O'), ('power', 'NN', 'B-NP'), ('in', 'IN', 'O'), ('the', 'DT', 'B-NP'), ('mobile', 'JJ', 'I-NP'), ('phone', 'NN', 'I-NP'), ('market', 'NN', 'B-NP'), ('and', 'CC', 'O'), ('ordered', 'VBD', 'O'), ('the', 'DT', 'B-NP'), ('company', 'NN', 'I-NP'), ('to', 'TO', 'O'), ('alter', 'VB', 'O'), ('its', 'PRP$', 'O'), ('practices', 'NNS', 'O')]\n"
     ]
    }
   ],
   "source": [
    "sentence_iob_tagged = tree2conlltags(cs)\n",
    "print(sentence_iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 IOB 객체는 리스트 튜플(원소가 튜플로 구성된 리스트) 구조라 list comprehension을 사용해서 해당 개체명을 추출해 낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['European']\n"
     ]
    }
   ],
   "source": [
    "query = [e1 for (e1, rel, e2) in sentence_iob_tagged if e2 in 'B-GPE']\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `ne_chunk` 개체명 인식\n",
    "\n",
    "`nltk` 라이브러리 `ne_chunk()` 함수를 사용해서 개체명을 인식시킬 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE European/JJ)\n",
      "  authorities/NNS\n",
      "  fined/VBD\n",
      "  (PERSON Google/NNP)\n",
      "  a/DT\n",
      "  record/NN\n",
      "  $/$\n",
      "  5.1/CD\n",
      "  billion/CD\n",
      "  on/IN\n",
      "  Wednesday/NNP\n",
      "  for/IN\n",
      "  abusing/VBG\n",
      "  its/PRP$\n",
      "  power/NN\n",
      "  in/IN\n",
      "  the/DT\n",
      "  mobile/JJ\n",
      "  phone/NN\n",
      "  market/NN\n",
      "  and/CC\n",
      "  ordered/VBD\n",
      "  the/DT\n",
      "  company/NN\n",
      "  to/TO\n",
      "  alter/VB\n",
      "  its/PRP$\n",
      "  practices/NNS)\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags, ne_chunk\n",
    "from pprint import pprint\n",
    "\n",
    "sentence_ne_tree = ne_chunk(sentence_pos)\n",
    "print(sentence_ne_tree) # 개체명 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Tree` 객체에서 개체명 추출\n",
    "\n",
    "문장에서 개체명 인식을 통해서 인식된 개체명만 추출하는 코드는 다음과 같다.\n",
    "즉, `nltk.ne_chunk()` 메쏘드는 `nltk.tree.Tree` 객체를 반환하기 때문에 `Tree`객체를 훑어서 인식된 개체명을 추출한다.\n",
    "\n",
    "-[stackoverflow, \"How can I extract GPE(location) using NLTK ne_chunk?\"](https://stackoverflow.com/questions/48660547/how-can-i-extract-gpelocation-using-nltk-ne-chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk import Tree\n",
    "\n",
    "def get_continuous_chunks(text, label):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for subtree in chunked:\n",
    "        if type(subtree) == Tree and subtree.label() == label:\n",
    "            current_chunk.append(\" \".join([token for token, pos in subtree.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return continuous_chunk\n",
    "\n",
    "get_continuous_chunks(sentence, 'GPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy NER\n",
    "\n",
    "`spacy`를 활용해서도 개체명 인식을 할 수 있다. Google이 NLTK라이브러리 `ne_chunk()`와 달리 제대로 인식된 것을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "European     \t NORP\n",
      "Google       \t ORG\n",
      "$5.1 billion \t MONEY\n",
      "Wednesday    \t DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "doc = nlp('European authorities fined Google a record $5.1 billion on Wednesday for abusing its power \\\n",
    "in the mobile phone market and ordered the company to alter its practices')\n",
    "for entity in doc.ents:\n",
    "    print(f'{entity.text:12} \\t {entity.label_}')\n",
    "# print([(X.text, X.label_) for X in doc.ents])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
