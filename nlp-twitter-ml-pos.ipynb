{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 트위터 감성분석 예측모형\n",
    "\n",
    "[트위터 감성 예측](nlp-twitter-ml.html)을 이어서 예측모형 개발을 계속 진행하는데 spaCy 처리작업에 시간이 많이 소요되어 데이터를 1,000건으로 줄인다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/twitter_sentiment_train.csv\", encoding = \"ISO-8859-1\")\n",
    "smpl_df = df.sample(1000, random_state = 77777)\n",
    "smpl_df.columns = [\"item_id\", \"sentiment\", \"text\"]\n",
    "smpl_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 품사(POS) 태깅\n",
    "\n",
    "한글의 경우 형태소로 쪼개서 토큰화하는 이유는 한국어에는 조사가 존재하여 영어에서 효과가 검증된 기법을 사용할 수 없다는 점이다.\n",
    "\n",
    "[\"Python으로 딥러닝하기 | 자연어 처리 1. 토크나이징\"](https://everyday-deeplearning.tistory.com/entry/Python으로-딥러닝하기-자연어-처리-1-토크나이징)에 나온 사례를 바탕으로 한 문장을 토큰화하는 과정을 비교해보자. 음절은 문자 단위, 어절은 단어 단위 (쉽게 띄어쓰기 단위), 형태소는 의미를 가진 가장 작은 단위로 쪼개는 것이다.\n",
    "\n",
    "- 예시 문장) \"한글 자연어 처리 공부는 어렵다\"\n",
    "    - 음절: 한, 글, 자, 연, 어, 처, 리, 공, 부, 는, 어, 렵, 다\n",
    "    - 어절: 한글, 자연어, 처리, 공부는, 어렵다\n",
    "    - 형태소: 한글, 자연어, 처리, 공부, 는, 어렵, 다\n",
    "\n",
    "텍스트 품사(POS) 태깅을 위해서 `NLTK`를 많이 사용했으나, 최근에는 spaCy로 넘어가고 있는 추세다. \n",
    "아나콘다를 기반으로 작업을 진행하는 경우 다음과 같이 `conda install` 명령어를 사용해서 spacy를 설치하면 된다.\n",
    "그리고 나서 영어 사전이 필요하니 `python -m spacy` 명령어로 영어를 사용할 수 있도록 설치한다.\n",
    "\n",
    "- `$ conda install -c conda-forge spacy` # conda config --add channels conda-forge\n",
    "- `$ python -m spacy download en`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy 들어가며\n",
    "\n",
    "영어 문장을 spaCy를 통해서 `Wall Street Journal just published an interesting piece on crypto currencies` 영어문장을 형태소분석 작업을 다음과 같이 수행할 수 있고, 이외에도 `token.dep_`, `oken.is_stop` 등도 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Text\tIndex\tLemma\tPUNCT\tAlpha\tShape\tPOS\tTAG\n",
      "================================================================================\n",
      "Wall\t0\tWall\tFalse\tFalse\tXxxx\tPROPN\tNNP\n",
      "Street\t5\tStreet\tFalse\tFalse\tXxxxx\tPROPN\tNNP\n",
      "Journal\t12\tJournal\tFalse\tFalse\tXxxxx\tPROPN\tNNP\n",
      "just\t20\tjust\tFalse\tFalse\txxxx\tADV\tRB\n",
      "published\t25\tpublish\tFalse\tFalse\txxxx\tVERB\tVBD\n",
      "an\t35\tan\tFalse\tFalse\txx\tDET\tDT\n",
      "interesting\t38\tinteresting\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "piece\t50\tpiece\tFalse\tFalse\txxxx\tNOUN\tNN\n",
      "on\t56\ton\tFalse\tFalse\txx\tADP\tIN\n",
      "crypto\t59\tcrypto\tFalse\tFalse\txxxx\tADJ\tJJ\n",
      "currencies\t66\tcurrency\tFalse\tFalse\txxxx\tNOUN\tNNS\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"==\"*40)\n",
    "print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "    \"Text\",\n",
    "    \"Index\",\n",
    "    \"Lemma\",\n",
    "    \"PUNCT\",\n",
    "    \"Alpha\",\n",
    "    \"Shape\",\n",
    "    \"POS\",\n",
    "    \"TAG\"))\n",
    "print(\"==\"*40)\n",
    "\n",
    "doc = nlp(\"Wall Street Journal just published an interesting piece on crypto currencies\")\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\\t{6}\\t{7}\".format(\n",
    "        token.text,\n",
    "        token.idx,\n",
    "        token.lemma_,\n",
    "        token.is_punct,\n",
    "        token.is_space,\n",
    "        token.shape_,\n",
    "        token.pos_,\n",
    "        token.tag_\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spaCy 병렬처리\n",
    "\n",
    "[stackoverflow, \"Applying Spacy Parser to Pandas DataFrame w/ Multiprocessing\"](https://stackoverflow.com/questions/44395656/applying-spacy-parser-to-pandas-dataframe-w-multiprocessing)를 참조하여 Spacy Parser를 데이터프레임에 멀티프로세싱으로 처리할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokens</th>\n",
       "      <th>text_lemma</th>\n",
       "      <th>text_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16392</th>\n",
       "      <td>16404</td>\n",
       "      <td>0</td>\n",
       "      <td>... Football....     Its gone!</td>\n",
       "      <td>[..., Football, ....,     , Its, gone, !]</td>\n",
       "      <td>[..., football, ....,     , -PRON-, go, !]</td>\n",
       "      <td>[PUNCT, NOUN, PUNCT, SPACE, DET, VERB, PUNCT]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92027</th>\n",
       "      <td>92039</td>\n",
       "      <td>0</td>\n",
       "      <td>@bar_bar_ella Unfortunately the thumb drive cr...</td>\n",
       "      <td>[@bar_bar_ella, Unfortunately, the, thumb, dri...</td>\n",
       "      <td>[@bar_bar_ella, unfortunately, the, thumb, dri...</td>\n",
       "      <td>[VERB, ADV, DET, NOUN, NOUN, VERB, PROPN, PROP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82269</th>\n",
       "      <td>82281</td>\n",
       "      <td>1</td>\n",
       "      <td>@ashleylynnangle Lol but that place is awesome...</td>\n",
       "      <td>[@ashleylynnangle, Lol, but, that, place, is, ...</td>\n",
       "      <td>[@ashleylynnangle, Lol, but, that, place, be, ...</td>\n",
       "      <td>[PUNCT, PROPN, CCONJ, DET, NOUN, VERB, ADJ, PU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83920</th>\n",
       "      <td>83932</td>\n",
       "      <td>0</td>\n",
       "      <td>@cateedelaloye miss you</td>\n",
       "      <td>[@cateedelaloye, miss, you]</td>\n",
       "      <td>[@cateedelaloye, miss, -PRON-]</td>\n",
       "      <td>[NOUN, VERB, PRON]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37637</th>\n",
       "      <td>37649</td>\n",
       "      <td>1</td>\n",
       "      <td>@allbloominwrong That is so sweet. Please say ...</td>\n",
       "      <td>[@allbloominwrong, That, is, so, sweet, ., Ple...</td>\n",
       "      <td>[@allbloominwrong, that, be, so, sweet, ., ple...</td>\n",
       "      <td>[PUNCT, DET, VERB, ADV, ADJ, PUNCT, INTJ, VERB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       item_id  sentiment                                               text  \\\n",
       "16392    16404          0                     ... Football....     Its gone!   \n",
       "92027    92039          0  @bar_bar_ella Unfortunately the thumb drive cr...   \n",
       "82269    82281          1  @ashleylynnangle Lol but that place is awesome...   \n",
       "83920    83932          0                           @cateedelaloye miss you    \n",
       "37637    37649          1  @allbloominwrong That is so sweet. Please say ...   \n",
       "\n",
       "                                             text_tokens  \\\n",
       "16392          [..., Football, ....,     , Its, gone, !]   \n",
       "92027  [@bar_bar_ella, Unfortunately, the, thumb, dri...   \n",
       "82269  [@ashleylynnangle, Lol, but, that, place, is, ...   \n",
       "83920                        [@cateedelaloye, miss, you]   \n",
       "37637  [@allbloominwrong, That, is, so, sweet, ., Ple...   \n",
       "\n",
       "                                              text_lemma  \\\n",
       "16392         [..., football, ....,     , -PRON-, go, !]   \n",
       "92027  [@bar_bar_ella, unfortunately, the, thumb, dri...   \n",
       "82269  [@ashleylynnangle, Lol, but, that, place, be, ...   \n",
       "83920                     [@cateedelaloye, miss, -PRON-]   \n",
       "37637  [@allbloominwrong, that, be, so, sweet, ., ple...   \n",
       "\n",
       "                                                text_pos  \n",
       "16392      [PUNCT, NOUN, PUNCT, SPACE, DET, VERB, PUNCT]  \n",
       "92027  [VERB, ADV, DET, NOUN, NOUN, VERB, PROPN, PROP...  \n",
       "82269  [PUNCT, PROPN, CCONJ, DET, NOUN, VERB, ADJ, PU...  \n",
       "83920                                 [NOUN, VERB, PRON]  \n",
       "37637  [PUNCT, DET, VERB, ADV, ADJ, PUNCT, INTJ, VERB...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = []\n",
    "lemma = []\n",
    "pos = []\n",
    "\n",
    "for doc in nlp.pipe(smpl_df['text'].astype('unicode').values, batch_size=50, n_threads=8):\n",
    "    if doc.is_parsed:\n",
    "        tokens.append([n.text for n in doc])\n",
    "        lemma.append([n.lemma_ for n in doc])\n",
    "        pos.append([n.pos_ for n in doc])\n",
    "    else:\n",
    "        # We want to make sure that the lists of parsed results have the\n",
    "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
    "        tokens.append(None)\n",
    "        lemma.append(None)\n",
    "        pos.append(None)\n",
    "\n",
    "smpl_df['text_tokens'] = tokens\n",
    "smpl_df['text_lemma'] = lemma\n",
    "smpl_df['text_pos'] = pos\n",
    "smpl_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
