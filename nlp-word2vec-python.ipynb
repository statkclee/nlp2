{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `word2vec` 들어가며\n",
    "\n",
    "[Minsuk Heo 허민석](https://www.youtube.com/channel/UCxP77kNgVfiiG6CXZ5WMuAQ) 유튜브 채널의 [딥러닝 자연어처리] Word2Vec 동영상을 살펴보면 워드2벡(word2vec)의 기본개념을 잡을 수 있다.\n",
    "\n",
    "\n",
    "- Chris McCormick (19 Apr 2016), “Word2Vec Tutorial - The Skip-Gram Model”\n",
    "- McCormick, C. (2016, April 19). Word2Vec Tutorial - The Skip-Gram Model. Retrieved from http://www.mccormickml.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"300\" height=\"180\" src=\"https://www.youtube.com/embed/sY4YyacSsLc\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 환경설정\n",
    "\n",
    "구글 뉴스 문서(Google News documents)가 가장 유명해서 [Mikolov](https://bit.ly/GoogleNews-vectors-negative300) 기학습된 모형을 받아 이를 활용한다. 다운로드 받는 사전학습 모형을 `.load_word2vec_format()` 메쏘드로 불러읽어들인다. 구글 뉴스 모형은 크기가 1.5GB로 단어가 3백만로 200차원으로 되어 있어 이를 32비트 컴퓨터에서는 열수가 없고 충분한 메모리가 확보되어야 가능하다.\n",
    "\n",
    "1.5GB 학습된 모형을 불러올 경우 메모리가 적은 컴퓨터의 경우 문제가 될 수 있어 `limit=300000` 인자를 넣어 다소 정확도는 희생하더라도 빠른 후속 작업이 되도록 설정을 변경시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "w2vec = KeyedVectors.load_word2vec_format(\\\n",
    "        './model/GoogleNews-vectors-negative300.bin.gz', binary=True, limit=900000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `word2vec` 유명한 사례\n",
    "`word2vec`의 유명한 사례로 많이 회자되는 `king` - `man` + `woman` 연산을 하게 되면 어떤 결과가 나오는지 직접 데이터를 넣어 확인해보자. 정답은 `queen`이 나와야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431607246399)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec.most_similar(positive=['king', 'woman'], negative=['man'], topn=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유사도\n",
    "\n",
    "두 단어간의 유사도는 `w2vec` 객체의 `.similarity()` 메쏘드를 사용해서 계산해도 된다. 혹은 아래와 같이 `numpy`를 사용해서 계산해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5648358846375967"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec.similarity('korea', 'japan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "두 단어간 유사도를 계산하는 방식은 다음 수식에 따라 `korea`, `japna`을 넣어주면 유사도를 구할 수 있다.\n",
    "\n",
    "\\begin{equation*}\n",
    "cos(\\pmb x, \\pmb y) = \\frac {\\pmb x \\cdot \\pmb y}{||\\pmb x|| \\cdot ||\\pmb y||}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56483585"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "cosine_similarity = numpy.dot(w2vec['korea'], w2vec['japan'])\\\n",
    "                    /(numpy.linalg.norm(w2vec['korea']) * \\\n",
    "                      numpy.linalg.norm(w2vec['japan']))\n",
    "cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec 활용사례\n",
    "\n",
    "`chef`와 유사한 단어 5개를 뽑아보자. `chef`는 우리가 친숙한 요리관련 요리사를 지칭할 수도 있지만, DevOps에서 소프트웨어를 지칭할 수도 있다. `negative`를 사용하게 되면 `food`관련 내용을 제외하고 관련 유사성이 높은 단어를 추출하는 것도 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chef', 0.823263943195343),\n",
       " ('pastry_chef', 0.7945826053619385),\n",
       " ('sous_chef', 0.793190598487854),\n",
       " ('chefs', 0.7798833250999451),\n",
       " ('Executive_Chef', 0.7338546514511108)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec.most_similar(positive=['chef'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Software', 0.48984354734420776),\n",
       " ('iLife_iWork', 0.47542810440063477),\n",
       " ('programmer', 0.4731772243976593),\n",
       " ('Qlusters', 0.4657154083251953),\n",
       " ('Pramati', 0.46331697702407837)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec.most_similar(positive=['chef', 'software'], negative=['food'], topn=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `word2vec` 경쟁자들\n",
    "\n",
    "구글에서 `Tomas Mikolov`와 동료들이 word2vec을 만들어 논문도 내고 훈련된 모형도 공개하는 동안 스탠포드대학 `Jeffrey Pennington`은 역전파(backpropagation)을 학습 알고리즘으로 사용하는 `word2vec`의 단점을 극복할 수 있는 GloVe(Global Vectors)를 출시하였다. SVD를 근간으로 하고 있으 역전파 알고리즘과 비교하여 성능이 대폭 향상되었다.\n",
    "페이스북도 `fastText`를 내놓으면서 인접 단어 대신 `n-문자`를 예측하는 것으로 바꿔 오탈자에 강건하면서, 영어뿐만 아니라 294개 언어로 학습된 모형을 내놓았다.\n",
    "\n",
    "따라서, 공부는 `word2vec`, 특정 영역에 활용될 `word2vec` 모형개발에는 `GloVe`를 다국어에는 `fastText`가 각자 장점을 내세우면서 가축을 벌이고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fastText`\n",
    "fastText 웹사이트 \n",
    "[Download &rarr; Word vectors for 157 languages](https://fasttext.cc/docs/en/crawl-vectors.html)에서 학습된 모형을 불러와서 특정 한글에 대해 유사 단어를 확인하는 것이 가능하다. 크기가 4.2GB라고 나옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "#fb_fastText = FastText.load_fasttext_format(model_file=\"\")\n",
    "#fb_fastText.most_similar('soccer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 추천시스템\n",
    "\n",
    "`word2vec` 혹은 `doc2vec`을 사용하여 상품/서비스/문서 추천시스템을 구축하는 것도 가능하다.\n",
    "\n",
    "- [Using Word2Vec to Create Recommendation Systems](https://blog.lipishala.com/2018/10/03/using-word2vec-to-create-recommendation-systems/)\n",
    "- [Word2Vec 그리고 \n",
    "추천 시스템의 Item2Vec](https://brunch.co.kr/@goodvc78/16)\n",
    "- [최규민, 추천시스템이 word2vec을 만났을때 - PyCon Korea 2015](https://www.youtube.com/watch?v=iutEgQg7yws"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
