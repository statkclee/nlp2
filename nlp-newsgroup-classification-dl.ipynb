{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기계학습(ML) &rarr; 딥러닝(DL)\n",
    "\n",
    "전통적인 통계기반의 예측모형을 사용하는 것이 아니라 최근에 개발된 딥러닝을 적용시켜 뉴스그룹(뉴스기사) 분류작업을 수행해보자.\n",
    "\n",
    "- 나이브 베이즈: MultinomialNB\n",
    "- 로지스틱 회귀모형: LogisticRegression\n",
    "- 선형 SVM: SVC\n",
    "- SGDClassifier\n",
    "- Random Forest: RandomForestClassifier\n",
    "- Gradient Boosted Machines: GradientBoostingClassifier\n",
    "\n",
    "전통적인 통계기반 예측모형 대신에 딥러닝에 포함되는 모형은 여러가지가 있지만 아무래도 **워드2벡(word2vec)** 을 가장 먼저 꼽고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "## 환경설정\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 저자 제작 사용자정의 함수 가져오기\n",
    "import sys\n",
    "sys.path.insert(0, './code')\n",
    "import text_normalizer as tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('data/clean_newsgroups.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12261,), (6039,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_corpus, test_corpus, train_label_nums, test_label_nums, train_label_names, test_label_names =\\\n",
    "                                 train_test_split(np.array(data_df['Clean Article']), np.array(data_df['Target Label']),\n",
    "                                                       np.array(data_df['Target Name']), test_size=0.33, random_state=42)\n",
    "\n",
    "train_corpus.shape, test_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train = [tn.tokenizer.tokenize(text)\n",
    "                   for text in train_corpus]\n",
    "tokenized_test = [tn.tokenizer.tokenize(text)\n",
    "                   for text in test_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측모형 - 워드2벡\n",
    "\n",
    "워드2벡(word2vec)이 포함된 라이브러리로 `gensim`이 선택되었다. 다음 명령어를 사용해서 `gensim` 라이브러리를 설치한다.\n",
    "\n",
    "> `$ pip install -U gensim` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "# build word2vec model\n",
    "w2v_num_features = 1000\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train, size=w2v_num_features, window=100,\n",
    "                                   min_count=2, sample=1e-3, sg=1, iter=5, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index2word)\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model.wv[word])\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate averaged word vector features from word2vec model\n",
    "avg_wv_train_features = document_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
    "                                                     num_features=w2v_num_features)\n",
    "avg_wv_test_features = document_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
    "                                                    num_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'Word2Vec model:> Train features shape: \\t {avg_wv_train_features.shape} \\n\n",
    "                         Test features shape: \\t {avg_wv_test_features.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측모형 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svm = SGDClassifier(loss='hinge', penalty='l2', random_state=42, max_iter=500)\n",
    "svm.fit(avg_wv_train_features, train_label_names)\n",
    "svm_w2v_cv_scores = cross_val_score(svm, avg_wv_train_features, train_label_names, cv=5)\n",
    "svm_w2v_cv_mean_score = np.mean(svm_w2v_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_w2v_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_w2v_cv_mean_score)\n",
    "svm_w2v_test_score = svm.score(avg_wv_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_w2v_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측모형 - GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering with GloVe model\n",
    "train_nlp = [tn.nlp(item) for item in train_corpus]\n",
    "train_glove_features = np.array([item.vector for item in train_nlp])\n",
    "\n",
    "test_nlp = [tn.nlp(item) for item in test_corpus]\n",
    "test_glove_features = np.array([item.vector for item in test_nlp])\n",
    "\n",
    "print('GloVe model:> Train features shape:', train_glove_features.shape, \n",
    "      ' Test features shape:', test_glove_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측모형 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SGDClassifier(loss='hinge', penalty='l2', random_state=42, max_iter=500)\n",
    "svm.fit(train_glove_features, train_label_names)\n",
    "svm_glove_cv_scores = cross_val_score(svm, train_glove_features, train_label_names, cv=5)\n",
    "svm_glove_cv_mean_score = np.mean(svm_glove_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_glove_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_glove_cv_mean_score)\n",
    "svm_glove_test_score = svm.score(test_glove_features, test_label_names)\n",
    "print('Test Accuracy:', svm_glove_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 예측모형 - FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "ft_num_features = 1000\n",
    "# sg decides whether to use the skip-gram model (1) or CBOW (0)\n",
    "ft_model = FastText(tokenized_train, size=ft_num_features, window=100, \n",
    "                    min_count=2, sample=1e-3, sg=1, iter=5, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate averaged word vector features from word2vec model\n",
    "avg_ft_train_features = document_vectorizer(corpus=tokenized_train, model=ft_model,\n",
    "                                                     num_features=ft_num_features)\n",
    "avg_ft_test_features = document_vectorizer(corpus=tokenized_test, model=ft_model,\n",
    "                                                    num_features=ft_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FastText model:> Train features shape:', avg_ft_train_features.shape, \n",
    "      ' Test features shape:', avg_ft_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측모형 성능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SGDClassifier(loss='hinge', penalty='l2', random_state=42, max_iter=500)\n",
    "svm.fit(avg_ft_train_features, train_label_names)\n",
    "svm_ft_cv_scores = cross_val_score(svm, avg_ft_train_features, train_label_names, cv=5)\n",
    "svm_ft_cv_mean_score = np.mean(svm_ft_cv_scores)\n",
    "print('CV Accuracy (5-fold):', svm_ft_cv_scores)\n",
    "print('Mean CV Accuracy:', svm_ft_cv_mean_score)\n",
    "svm_ft_test_score = svm.score(avg_ft_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_ft_test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(solver='adam', alpha=1e-5, learning_rate='adaptive', early_stopping=True,\n",
    "                    activation = 'relu', hidden_layer_sizes=(512, 512), random_state=42)\n",
    "mlp.fit(avg_ft_train_features, train_label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_ft_test_score = mlp.score(avg_ft_test_features, test_label_names)\n",
    "print('Test Accuracy:', svm_ft_test_score)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
