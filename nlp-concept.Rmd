---
title: "자연어처리 (NLP)"
subtitle: "자연어처리 핵심 개념"
author:
    name: "[Tidyverse Korea](https://web.facebook.com/groups/tidyverse/)"
date: "`r Sys.Date()`"
output:
  html_document: 
    toc: yes
    toc_float: true
    highlight: tango
    code_folding: show
    number_section: true
    self_contained: true
editor_options: 
  chunk_output_type: console
---


``` {r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE,
                    comment="", digits = 3, tidy = FALSE, prompt = FALSE, fig.align = 'center')

```

# 핵심 개념 {#basic-cocept}

자연어 처리는 컴퓨터로 하여금 사람이 작성한 언어(음성과 글)의 의미를 이해시키는 것을 목표로 한다.

1. 텍스트 데이터
    - 트위터
    - 소설, 신문기사
    - 고객 평점과 리뷰
    - 전자우편
    - 의무기록
1. 텍스트 저장 형식
    - 뉴스 등 웹 페이지
    - PDF/워드/한글 문서
    - 트위터 등 SNS, RSS 피드, 댓글
    - ...
1. 응용분야
    - 감성분석
    - 텍스트 분류 
    - 번역
    - 챗봇
    - 개인 비서
    - ...
1. 기술
    - 단어주머니(Bag of Words)
    - Word Embedding, 워드투벡(Word2Vec)


# 단어주머니(Bag of Words) {#python-bow}

## 영어 단어주머니(Bag of Words) [^bow-from-scratch] {#python-bow-english}

[^bow-from-scratch]: [Usman Malik(July 04, 2019), Python for NLP: Creating Bag of Words Model from Scratch](https://stackabuse.com/python-for-nlp-creating-bag-of-words-model-from-scratch/)

먼저 텍스트 데이터를 준비한다. 3개 문장으로 구성된 영어문장을 텍스트 데이터로 삼아 단어주머니 언어모형을 구현해보자. `text`는 하나의 텍스트이며, `sentence`는 문장 3개를 들고 있는 리스트다.

```{python text-data}
text = "I like to play football. Did you go outside to play tennis. John and I play tennis"
sentence = ["I like to play football.", "Did you go outside to play tennis.", "John and I play tennis"]
```

단어주머니 모형은 오래되고 직관적이며 오래되서 잘 구현된 라이브러리가 많이 있다. `nltk`, `TextBlob` 라이브러리를 사용해서 단어주머니 모형을 구현해보자.
"아나콘다3"을 기반으로 작업할 경우 설치가 되어 있지 않으니 `conda`를 사용해서 설치한다. 터미널에서 `conda install -c conda-forge textblob` 명령어를 실행시키면 [`TextBlob`이 설치](https://anaconda.org/conda-forge/textblob)된다.

기본적인 작업흐름은 텍스트 데이터를 `blob` 객체로 변환시킨다. 그리고 나서 `blob` 객체 메쏘드를 사용해서 단어주머니를 만들고 이를 판다스 데이터프레임으로 변환시킨 후에 `groupby()` 메쏘드를 사용해서 단어 빈도수를 만든다.

```{python text-bow-textblob}
from textblob import TextBlob
import pandas as pd

blob = TextBlob(text)
print(blob.words) 

# 데이터프레임 변환
text_df = pd.DataFrame(blob.pos_tags, columns=['word', 'pos'])

text_df.groupby('word')['word'].count().sort_values(ascending=False)
```

["Sentiment Analysis using Python"](https://data-science-blog.com/blog/2018/11/04/sentiment-analysis-using-python/)은 데이터프레임 판다스를 기본 자료구조로 감성분석하는 좋은 코드를 만들어 주었다. 앞선 3문장을 대상으로 간단히 감성분석 작업을 수행해보자!

```{python text-bow-textblob-sentiment}
sentence_df = pd.DataFrame(sentence, columns=['sentence'])
sentence_df['sentiment'] = sentence_df.sentence.apply(lambda sentence: TextBlob(sentence).sentiment)

print(sentence_df)
```

명사만 추출하고자 하는 경우 앞서 단어에 대한 품사를 식별했기 때문에 판다스 행을 추출하는 방식으로 뽑아낸다.

```{python text-bow-textblob-noun}
text_df[text_df['pos'] == "NN"]
```


# NLP 도구 {#nlp-tools}

## 영문 NLP 도구 [^nlp-comparison] {#nlp-tools-english}

[^nlp-comparison]: [Analytics Vidhya, "Natural Language Processing Made Easy - using SpaCy (in Python)"](https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/)

영어 자연어처리를 위해서 다양한 파이썬 라이브러리가 존재한다. 대표적으로 많이 사용되는 자연어 처리 파이썬 라이브러리는 다음과 같다.

- Spacy: https://spacy.io/, MIT License
- NLTK: https://www.nltk.org/, Apache 2.0
- CoreNLP: https://stanfordnlp.github.io/CoreNLP/index.html, GPL v3
    
영문 자연어 처리에서 많이 사용되는 `Spacy`, `NLTK`, `CoreNLP`의 경우 라이선스에서 차이도 나지만 기능에서도 차이가 있다.

| Feature | Spacy | NLTK | Core NLP |
|---------|:--------:|:---------:|:---------:|:---------:|
| Easy installation | Y | Y | Y |
| Python API | Y | Y | N |
| Multi Language support | N | Y |Y |
| Tokenization | Y | Y | Y | |
| Part-of-speech tagging | Y | Y | Y |
| Sentence segmentation | Y | Y | Y |
| Dependency parsing | Y | N | Y |
| Entity Recognition | Y | Y | Y |
| Integrated word vectors | Y | N | N |
| Sentiment analysis | Y | Y | Y |
| Coreference resolution | N | N | Y |
    
## 한국어 단어주머니 {#bow-korean}


# 워드 임베딩 {#word-embedding}


