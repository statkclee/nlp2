{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "트위터 감성을 분류하는데 트위터 텍스트를 사전에 정제할 필요가 있다. \n",
    "이를 위해서 `spacy` 라이브러리를 활용하는데 `clean_text()` 사용자 정의 함수를 만들어서 표제어 추출(lemmatization) 작업을 수행해서 이를 BoW(Bag of Words)에 넣어 트위터 감성 분류하는데 사용할 예측모형을 개발한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트위터 데이터 가져오기\n",
    "\n",
    "먼저 트위터 텍스트 데이터를 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/twitter_sentiment_train.csv\", encoding = \"ISO-8859-1\")\n",
    "smpl_df = df.sample(1000, random_state = 77777)\n",
    "smpl_df.columns = [\"item_id\", \"sentiment\", \"text\"]\n",
    "smpl_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 정제\n",
    "\n",
    "텍스트를 정제하는 기법은 여러가지가 있지만, 아래 `clean_text()` 사용자 정의 함수를 만들어서 표제어 추출(lemmatization) 작업을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Version: 2.1.4\n",
      "16392                                             football\n",
      "92027    unfortunately thumb drive crash Windows Explor...\n",
      "82269                    Lol place awesome buy annual pass\n",
      "83920                                                 miss\n",
      "37637                                     sweet hello long\n",
      "Name: lemma_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "print(f\"spaCy Version: {spacy.__version__}\")\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# 데이터 전처리 함수\n",
    "def clean_text(text):\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in spacy_stopwords]\n",
    "    return ' '.join(a_lemmas)\n",
    "  \n",
    "smpl_df['lemma_text'] = smpl_df['text'].apply(clean_text)\n",
    "print(smpl_df['lemma_text'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW 모형\n",
    "\n",
    "BoW 모형을 `sklearn` 라이브러리 `CountVectorizer` 클래스를 사용해서 구현한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `CountVectorizer` 클래스\n",
    "\n",
    "`CountVectorizer` 클래스를 활용하여 앞서 표제어 추출한 칼럼('lemma_text')을 BoW 객체로 변환시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2325)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "\n",
    "twitter_bow = vectorizer.fit_transform(smpl_df['lemma_text'])\n",
    "\n",
    "print(twitter_bow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW 변환이 제대로 되었는지를 확인하기 위해서 BoW를 데이터프레임으로 변환시킨다.\n",
    "그리고 나서 `get_feature_names()` 메쏘드를 호출해서 변환시킨 판다스 데이터프레임 칼럼명으로 넣어 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   aarooonnnn  abandon  able  absolute  absolutely  access  accompany  \\\n",
      "0           0        0     0         0           0       0          0   \n",
      "1           0        0     0         0           0       0          0   \n",
      "2           0        0     0         0           0       0          0   \n",
      "3           0        0     0         0           0       0          0   \n",
      "4           0        0     0         0           0       0          0   \n",
      "\n",
      "   account  ace  ache  ...    yu  yuk  yummmm  yummy  yung  yup  zac  zealand  \\\n",
      "0        0    0     0  ...     0    0       0      0     0    0    0        0   \n",
      "1        0    0     0  ...     0    0       0      0     0    0    0        0   \n",
      "2        0    0     0  ...     0    0       0      0     0    0    0        0   \n",
      "3        0    0     0  ...     0    0       0      0     0    0    0        0   \n",
      "4        0    0     0  ...     0    0       0      0     0    0    0        0   \n",
      "\n",
      "   zombie  zurah  \n",
      "0       0      0  \n",
      "1       0      0  \n",
      "2       0      0  \n",
      "3       0      0  \n",
      "4       0      0  \n",
      "\n",
      "[5 rows x 2325 columns]\n"
     ]
    }
   ],
   "source": [
    "twitter_bow_df = pd.DataFrame(twitter_bow.toarray())\n",
    "\n",
    "twitter_bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "print(twitter_bow_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 나이브 베이즈 예측모형\n",
    "\n",
    "표제어 추출을 통해서 나름 전처리가 완료된 트위터 텍스트를 `BoW` 모형으로 Feature를 추출해 낼 수 있는 준비가 완료되었기에 다음 단계로 나이브 베이즈 예측모형에 넣어 예측모형의 성능을 비교해 본다.\n",
    "\n",
    "먼저 훈련/시험 데이터로 나누고 앞서 정제한 표제어 추출 칼럼('lemma_text')을 `CountVectorizer` 클래스를 통해서 BoW 객체로 만들고 이를 나이브 베이즈로 예측정확도를 높여본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시험 데이터셋으로 검정한 감성 분류 예측모형 정확도: 0.657\n"
     ]
    }
   ],
   "source": [
    "# 나이브 베이즈 모형\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 훈련/시험 데이터 분리\n",
    "feature_df = smpl_df[['lemma_text']]\n",
    "\n",
    "target = smpl_df['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test  = train_test_split(feature_df, target, test_size=0.3)\n",
    "\n",
    "# BoW Feature \n",
    "vect = CountVectorizer(analyzer='word')\n",
    "\n",
    "X_train_bow = vect.fit_transform(X_train['lemma_text'])\n",
    "X_test_bow = vect.transform(X_test['lemma_text'])\n",
    "\n",
    "# 예측모형 적합\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "clf.fit(X_train_bow, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "accuracy = clf.score(X_test_bow, y_test)\n",
    "print(f\"시험 데이터셋으로 검정한 감성 분류 예측모형 정확도: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 통계 지표 + BoW\n",
    "\n",
    "[트위터 감성 예측 - 텍스트 통계지표](nlp-twitter-ml) 모형에 BoW를 결합시켜 예측모형의 정확도를 높여본다.\n",
    "\n",
    "## 텍스트 통계 지표\n",
    "\n",
    "텍스트 통계 지표를 [트위터 감성 예측 - 텍스트 통계지표](nlp-twitter-ml)에서 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma_text</th>\n",
       "      <th>char_cnt</th>\n",
       "      <th>hashtag_cnt</th>\n",
       "      <th>word_cnt</th>\n",
       "      <th>mention_cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16392</th>\n",
       "      <td>football</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92027</th>\n",
       "      <td>unfortunately thumb drive crash Windows Explor...</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82269</th>\n",
       "      <td>Lol place awesome buy annual pass</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83920</th>\n",
       "      <td>miss</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37637</th>\n",
       "      <td>sweet hello long</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              lemma_text  char_cnt  \\\n",
       "16392                                           football         8   \n",
       "92027  unfortunately thumb drive crash Windows Explor...        67   \n",
       "82269                  Lol place awesome buy annual pass        33   \n",
       "83920                                               miss         4   \n",
       "37637                                   sweet hello long        16   \n",
       "\n",
       "       hashtag_cnt  word_cnt  mention_cnt  \n",
       "16392            0         1            0  \n",
       "92027            0        10            0  \n",
       "82269            0         6            0  \n",
       "83920            0         1            0  \n",
       "37637            0         3            0  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################################################################\n",
    "# 텍스트 통계지표\n",
    "####################################################################\n",
    "# 1. 문자갯수\n",
    "# df['char_cnt'] = df['text'].apply(len)\n",
    "\n",
    "# 2. 단어갯수\n",
    "def count_words(text):\n",
    "    ''' \n",
    "        문장을 입력받아 단어갯수를 반환한다.\n",
    "    '''\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "# 3. 해쉬태그(#) 갯수\n",
    "def count_hashtag(text):\n",
    "    ''' \n",
    "        문장을 입력받아 해쉬태그(#)를 센다\n",
    "    '''\n",
    "    words = text.split()\n",
    "    \n",
    "    hashtags = [word for word in words if word.startswith('#')]\n",
    "    return len(hashtags)\n",
    "\n",
    "# 4. 언급(@) 갯수\n",
    "def count_mention(text):\n",
    "    ''' \n",
    "        문장을 입력받아 언급횟수(@)를 센다\n",
    "    '''\n",
    "    words = text.split()\n",
    "    \n",
    "    mentions = [word for word in words if word.startswith('@')]\n",
    "    return len(mentions)\n",
    "\n",
    "feature_df = smpl_df[['lemma_text']]\n",
    "\n",
    "feature_df['char_cnt'] = feature_df['lemma_text'].apply(len)\n",
    "feature_df['hashtag_cnt'] = feature_df['lemma_text'].apply(count_hashtag)\n",
    "feature_df['word_cnt'] = feature_df['lemma_text'].apply(count_words)\n",
    "feature_df['mention_cnt'] = feature_df['lemma_text'].apply(count_mention)\n",
    "\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW 모형\n",
    "\n",
    "BoW 모형을 다시 나이브베이즈 예측모형과 결합시켜 예측력을 높여본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test  = train_test_split(feature_df, target, test_size=0.3)\n",
    "\n",
    "# BoW Feature \n",
    "vect = CountVectorizer(analyzer='word')\n",
    "\n",
    "X_train_bow = vect.fit_transform(X_train['lemma_text'])\n",
    "X_test_bow = vect.transform(X_test['lemma_text'])\n",
    "\n",
    "X_train_vect_df = pd.DataFrame(X_train_bow.todense(), columns=vect.get_feature_names())\n",
    "X_train_vect_df = pd.concat([X_train.drop(['lemma_text'], axis=1), X_train_vect_df], axis=1)\n",
    "X_train_vect_df = X_train_vect_df.fillna(0)\n",
    "\n",
    "X_test_vect_df = pd.DataFrame(X_test_bow.todense(), columns=vect.get_feature_names())\n",
    "X_test_vect_df = pd.concat([X_test.drop(['lemma_text'], axis=1), X_test_vect_df], axis=1)\n",
    "X_test_vect_df = X_test_vect_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1392, 1828) (300, 1824)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vect_df.shape, X_test_vect_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1392, 700]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-027537d39144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m# Fit the classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_vect_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[1;31m# Measure the accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\statkclee\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \"\"\"\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\statkclee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\statkclee\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 205\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1392, 700]"
     ]
    }
   ],
   "source": [
    "# 예측모형 적합\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# Fit the classifier\n",
    "#clf.fit(X_train_vect_df, y_train)\n",
    "\n",
    "# Measure the accuracy\n",
    "#accuracy = clf.score(X_test_vect_df, y_test)\n",
    "print(f\"시험 데이터셋으로 검정한 감성 분류 예측모형 정확도: {accuracy:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
