{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `textblob` - 객체지향 NLP 라이브러리\n",
    "\n",
    "`textblob`은 `NLTK`를 기반으로 하여 텍스트 처리를 수월하게 할 수 있도록 다양한 기능을 많이 포함하고 있다.\n",
    "[textblob](https://textblob.readthedocs.io/en/latest/) 웹사이틀 통해서 소개에 나와 있듯이 \"Simplified Text Processing\"을 모토로 TextBlob 객체를 생성시키면 주요 메쏘드를 통해서 텍스트 처리 작업이 단순해 진다.\n",
    "\n",
    "- Noun phrase extraction\n",
    "- Part-of-speech tagging\n",
    "- Sentiment analysis\n",
    "- Classification (Naive Bayes, Decision Tree)\n",
    "- Language translation and detection powered by Google Translate\n",
    "- Tokenization (splitting text into words and sentences)\n",
    "- Word and phrase frequencies\n",
    "- Parsing\n",
    "- n-grams\n",
    "- Word inflection (pluralization and singularization) and lemmatization\n",
    "- Spelling correction\n",
    "- Add new models or languages through extensions\n",
    "- WordNet integration\n",
    "\n",
    "**참고문헌**\n",
    "\n",
    "- [SHUBHAM JAIN, (FEBRUARY 11, 2018), \"Natural Language Processing for Beginners: Using TextBlob\", Analytics Vidhya](https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 설치\n",
    "\n",
    "`textblob` 라이브러리를 사용하려면 우선 라이브러리를 먼저 설치하고, TextBlob에서 사용되는 NLTK 말뭉치(corpora)도 설치해야 된다.\n",
    "\n",
    "`conda`를 사용해서 다음 명령어로 `textblob` 라이브러리를 설치할 수 있다.\n",
    "\n",
    "`$ conda install -c conda-forge textblob`\n",
    "\n",
    "NLTK 말뭉치(corpora)도 다음 명령어를 사용해서 설치한다.\n",
    "\n",
    "- Brown Corpus: 품사 태깅(Part-of-speech Tagging)\n",
    "- Punkt: 영문 문장 토큰화\n",
    "- WordNet: 단어 정의, 유사어(synonyms)와 반의어(antonyms)\n",
    "- Averaged Perceptron Tagger: 품사 태깅(Part-of-speech Tagging)\n",
    "- conll2000: 텍스트를 명사, 동사 등으로 컴포넌트화.\n",
    "- Movie Reviews: 감성분석\n",
    "\n",
    "`$ ipython -m textblob.download_corpora`\n",
    "\n",
    "> `$ ipython -m textblob.download_corpora\n",
    "> [nltk_data] Downloading package brown to\n",
    "> [nltk_data]   Package brown is already up-to-date!\n",
    "> [nltk_data] Downloading package punkt to\n",
    "> [nltk_data]   Package punkt is already up-to-date!\n",
    "> [nltk_data] Downloading package wordnet to\n",
    "> [nltk_data]   Package wordnet is already up-to-date!\n",
    "> [nltk_data] Downloading package averaged_perceptron_tagger to\n",
    "> [nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
    "> [nltk_data] Downloading package conll2000 to\n",
    "> [nltk_data]   Package conll2000 is already up-to-date!\n",
    "> [nltk_data] Downloading package movie_reviews to\n",
    "> [nltk_data]   Package movie_reviews is already up-to-date!\n",
    "> Finished.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# textblob 헬로월드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 감성점수 0.06000000000000001 : \n",
      "The titular threat of The Blob has always struck me as the ultimate movie\n",
      "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
      "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
      "describes it--\"assimilating flesh on contact.\n",
      "- 감성점수 -0.34166666666666673 : Snide comparisons to gelatin be damned, it's a concept with the most\n",
      "devastating of potential consequences, not unlike the grey goo scenario\n",
      "proposed by technological theorists fearful of\n",
      "artificial intelligence run rampant.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "text = '''\n",
    "The titular threat of The Blob has always struck me as the ultimate movie\n",
    "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
    "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
    "describes it--\"assimilating flesh on contact.\n",
    "Snide comparisons to gelatin be damned, it's a concept with the most\n",
    "devastating of potential consequences, not unlike the grey goo scenario\n",
    "proposed by technological theorists fearful of\n",
    "artificial intelligence run rampant.\n",
    "'''\n",
    "\n",
    "blob = TextBlob(text)\n",
    "\n",
    "for sentence in blob.sentences:\n",
    "    print(f\"- 감성점수 {sentence.sentiment.polarity} : {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['The', 'titular', 'threat', 'of', 'The', 'Blob', 'has', 'always', 'struck', 'me', 'as', 'the', 'ultimate', 'movie', 'monster', 'an', 'insatiably', 'hungry', 'amoeba-like', 'mass', 'able', 'to', 'penetrate', 'virtually', 'any', 'safeguard', 'capable', 'of', 'as', 'a', 'doomed', 'doctor', 'chillingly', 'describes', 'it', 'assimilating', 'flesh', 'on', 'contact', 'Snide', 'comparisons', 'to', 'gelatin', 'be', 'damned', 'it', \"'s\", 'a', 'concept', 'with', 'the', 'most', 'devastating', 'of', 'potential', 'consequences', 'not', 'unlike', 'the', 'grey', 'goo', 'scenario', 'proposed', 'by', 'technological', 'theorists', 'fearful', 'of', 'artificial', 'intelligence', 'run', 'rampant'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJ</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNP</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNS</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRP</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RB</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBS</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VB</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBG</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBN</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBZ</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word\n",
       "pos      \n",
       "DT      9\n",
       "IN     10\n",
       "JJ     12\n",
       "NN     16\n",
       "NNP     1\n",
       "NNS     3\n",
       "PRP     3\n",
       "RB      5\n",
       "RBS     1\n",
       "TO      2\n",
       "VB      3\n",
       "VBG     1\n",
       "VBN     3\n",
       "VBZ     3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.tags[:5]\n",
    "# [('The', 'DT'),\n",
    "#  ('titular', 'JJ'),\n",
    "#  ('threat', 'NN'),\n",
    "#  ('of', 'IN'),\n",
    "#  ('The', 'DT')]\n",
    "text_df = pd.DataFrame(blob.tags, columns=['word', 'pos'])\n",
    "\n",
    "text_df.groupby('pos').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['titular threat', 'blob', 'ultimate movie monster', 'amoeba-like mass', 'snide', 'potential consequences', 'grey goo scenario', 'technological theorists fearful', 'artificial intelligence run rampant'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet 사전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a youthful male person',\n",
       " 'a friendly informal reference to a grown man',\n",
       " 'a male human offspring',\n",
       " '(ethnic slur) offensive and disparaging term for Black man']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "from textblob.wordnet import VERB\n",
    "word = Word(\"boy\")\n",
    "word.definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동의어(synonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('male_child.n.01'),\n",
       " Synset('boy.n.02'),\n",
       " Synset('son.n.01'),\n",
       " Synset('boy.n.04')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boy', 'male_child', 'son'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = set()\n",
    "for synset in word.synsets:\n",
    "    for lemma in synset.lemmas():\n",
    "        synonyms.add(lemma.name())\n",
    "        \n",
    "print(synonyms)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 반의어(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('male_child.n.01.male_child'), Lemma('male_child.n.01.boy')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = word.synsets[0].lemmas()\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('female_child.n.01.female_child')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[0].antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 언어 탐지와 번역\n",
    "\n",
    "텍스트를 입력값으로 넣게 되면 어떤 언어인지를 파악하는 것이 후속 자연어 처리 작업에서 매우 중요한 역할을 수행하게 된다. 이를 위해서 먼저 확인된 언어가 어느 나라 언어인지 결과값이 [구글 AdWords API 언어 코드](https://developers.google.com/adwords/api/docs/appendix/languagecodes?hl=ko)에 정리되어 있다.\n",
    "\n",
    "| LanguageName | LanguageCode  | CriteriaId |\n",
    "|--------------|---------------|--------------|\n",
    "| Arabic | ar  | 1019 |\n",
    "| Bulgarian | bg  | 1020 |\n",
    "| Catalan | ca  | 1038 |\n",
    "| Chinese (simplified )\t| zh_CN | 1017 |\n",
    "| Chinese (traditional)\t | zh_TW |\t1018 |\n",
    "| Croatian | hr  | 1039 |\n",
    "| Czech | cs  | 1021 |\n",
    "| Danish | da  | 1009 |\n",
    "| Dutch | nl  | 1010 |\n",
    "| English | en  | 1000 |\n",
    "| **Korean** | ko | 1012 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 언어 탐지\n",
    "\n",
    "구글을 통해 비즈니스통번역(영어) 3급 예제로 나온 한 문장을 받아 모범 번역과 얼마나 차이가 나는지 확인해보자.\n",
    "이를 위해서 먼저 텍스트가 어떤 언어인지 확인하는 과정과 번역하는 과정을 거쳐보자.\n",
    "\n",
    "> I am writing to thank you for your hospitality in my unexpected visit to your house. <br>\n",
    "> [모범번역] 통보 없이 방문했음에도 호의를 보여 주신 것에 감사 드리고자 글을 씁니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ko'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "korean_text = \"통보 없이 방문했음에도 호의를 보여 주신 것에 감사 드리고자 글을 씁니다.\"\n",
    "\n",
    "ko_blob = TextBlob(korean_text)\n",
    "ko_blob.detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"I would like to thank you for your kindness even though I visited without any notice.\")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ko_blob.translate(to='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 맞춤법 검사\n",
    "\n",
    "뉴스기사에도 최근에는 오탈자가 눈에 띄기 시작했다. 여러가지 이유가 있기는 하겠지만, 뉴스기사 절대량이 많아지는 것이 가장 큰 이유가 될 듯 싶다. 이유가 무엇이든간에 자연어 처리로 들어가기 전에 맞춤법 검사를 통해서 텍스트에 대한 품질을 높이는 작업이 필요한데 `TextBlob`에 포함된 기능을 사용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analytics Vidhya is a great platform to learn data science. \n",
      "\n",
      "When I grow up, I want to be a technincian! \n",
      "\n",
      "Of you think about it, it is original. \n",
      "\n",
      "Take one capsule by mouth nightly 3 hours before did. \n",
      "\n",
      "Violators will be bowed and find $50.\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "\n",
    "typo_sentences = '''\n",
    "Analytics Vidhya is a gret platfrm to learn data scence. \\n\n",
    "When I grow up, I want to be a technincian! \\n\n",
    "If you think about it, it is orignal. \\n\n",
    "Take one capsule by mouth nightly 3 hours before ded. \\n\n",
    "Violators will be towed and find $50.\n",
    "'''\n",
    "\n",
    "for line in typo_sentences.splitlines():\n",
    "    print(TextBlob(line).correct())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장의 각 단어별로 오탈자에 대한 수정단어를 확률값과 결합하여 튜플형태로 제시하여 준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Analytics', 0.0)]\n",
      "[('Vidhya', 0.0)]\n",
      "[('is', 1.0)]\n",
      "[('a', 1.0)]\n",
      "[('great', 0.5351351351351351), ('get', 0.3162162162162162), ('grew', 0.11216216216216217), ('grey', 0.026351351351351353), ('greet', 0.006081081081081081), ('fret', 0.002702702702702703), ('grit', 0.0006756756756756757), ('cret', 0.0006756756756756757)]\n",
      "[('platform', 1.0)]\n",
      "[('to', 1.0)]\n",
      "[('learn', 1.0)]\n",
      "[('data', 1.0)]\n",
      "[('science', 0.41379310344827586), ('scene', 0.33793103448275863), ('sciences', 0.10344827586206896), ('scenes', 0.08275862068965517), ('scented', 0.041379310344827586), ('spence', 0.020689655172413793)]\n"
     ]
    }
   ],
   "source": [
    "sample_sentence = typo_sentences.splitlines()[1]\n",
    "# Analytics Vidhya is a gret platfrm to learn data scence.\n",
    "\n",
    "sample_words = sample_sentence.split()\n",
    "\n",
    "for word in sample_words:\n",
    "    word_prob = Word(word).spellcheck()\n",
    "    print(word_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트 요약\n",
    "\n",
    "텍스트 요약(Text Summarization)하는 단순한 기법 중 하나는 텍스트에서 명사를 추출하는 것이다.\n",
    "\n",
    "- [A curated list of resources dedicated to text summarization](https://github.com/mathsyouth/awesome-text-summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is about...\n",
      "industries\n",
      "forums\n",
      "platforms\n",
      "communities\n",
      "platforms\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "blob = TextBlob('Analytics Vidhya is a thriving community for data driven industry. This platform allows \\\n",
    "people to know more about analytics from its articles, Q&A forum, and learning paths. Also, we help \\\n",
    "professionals & amateurs to sharpen their skillsets by providing a platform to participate in Hackathons.')\n",
    "\n",
    "nouns = list()\n",
    "for word, tag in blob.tags:\n",
    "    if tag == 'NN':\n",
    "        nouns.append(word.lemmatize())\n",
    "\n",
    "print (\"This text is about...\")\n",
    "for item in random.sample(nouns, 5):\n",
    "    word = Word(item)\n",
    "    print (word.pluralize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 감성분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련/시험 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = [\n",
    "    ('Tom Holland is a terrible spiderman.','pos'),\n",
    "    ('a terrible Javert (Russell Crowe) ruined Les Miserables for me...','pos'),\n",
    "    ('The Dark Knight Rises is the greatest superhero movie ever!','neg'),\n",
    "    ('Fantastic Four should have never been made.','pos'),\n",
    "    ('Wes Anderson is my favorite director!','neg'),\n",
    "    ('Captain America 2 is pretty awesome.','neg'),\n",
    "    ('Let\\s pretend \"Batman and Robin\" never happened..','pos'),\n",
    "]\n",
    "testing = [\n",
    "    ('Superman was never an interesting character.','pos'),\n",
    "    ('Fantastic Mr Fox is an awesome film!','neg'),\n",
    "    ('Dragonball Evolution is simply terrible!!','pos')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기계학습 예측모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측모형 성능: 1.00\n"
     ]
    }
   ],
   "source": [
    "from textblob import classifiers\n",
    "\n",
    "classifier = classifiers.NaiveBayesClassifier(training)\n",
    "\n",
    "print (f'예측모형 성능: {classifier.accuracy(testing):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 중요 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "            contains(is) = True              neg : pos    =      2.9 : 1.0\n",
      "         contains(never) = False             neg : pos    =      1.8 : 1.0\n",
      "             contains(a) = False             neg : pos    =      1.8 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_informative_features(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob('the weather is terrible!', classifier=classifier)\n",
    "print (blob.classify())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
