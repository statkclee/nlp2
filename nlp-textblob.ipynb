{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `textblob` - 객체지향 NLP 라이브러리\n",
    "\n",
    "`textblob`은 `NLTK`를 기반으로 하여 텍스트 처리를 수월하게 할 수 있도록 다양한 기능을 많이 포함하고 있다.\n",
    "[textblob](https://textblob.readthedocs.io/en/latest/) 웹사이틀 통해서 소개에 나와 있듯이 \"Simplified Text Processing\"을 모토로 TextBlob 객체를 생성시키면 주요 메쏘드를 통해서 텍스트 처리 작업이 단순해 진다.\n",
    "\n",
    "- Noun phrase extraction\n",
    "- Part-of-speech tagging\n",
    "- Sentiment analysis\n",
    "- Classification (Naive Bayes, Decision Tree)\n",
    "- Language translation and detection powered by Google Translate\n",
    "- Tokenization (splitting text into words and sentences)\n",
    "- Word and phrase frequencies\n",
    "- Parsing\n",
    "- n-grams\n",
    "- Word inflection (pluralization and singularization) and lemmatization\n",
    "- Spelling correction\n",
    "- Add new models or languages through extensions\n",
    "- WordNet integration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 설치\n",
    "\n",
    "`textblob` 라이브러리를 사용하려면 우선 라이브러리를 먼저 설치하고, TextBlob에서 사용되는 NLTK 말뭉치(corpora)도 설치해야 된다.\n",
    "\n",
    "`conda`를 사용해서 다음 명령어로 `textblob` 라이브러리를 설치할 수 있다.\n",
    "\n",
    "`$ conda install -c conda-forge textblob`\n",
    "\n",
    "NLTK 말뭉치(corpora)도 다음 명령어를 사용해서 설치한다.\n",
    "\n",
    "- Brown Corpus: 품사 태깅(Part-of-speech Tagging)\n",
    "- Punkt: 영문 문장 토큰화\n",
    "- WordNet: 단어 정의, 유사어(synonyms)와 반의어(antonyms)\n",
    "- Averaged Perceptron Tagger: 품사 태깅(Part-of-speech Tagging)\n",
    "- conll2000: 텍스트를 명사, 동사 등으로 컴포넌트화.\n",
    "- Movie Reviews: 감성분석\n",
    "\n",
    "`$ ipython -m textblob.download_corpora`\n",
    "\n",
    "> `$ ipython -m textblob.download_corpora\n",
    "> [nltk_data] Downloading package brown to\n",
    "> [nltk_data]   Package brown is already up-to-date!\n",
    "> [nltk_data] Downloading package punkt to\n",
    "> [nltk_data]   Package punkt is already up-to-date!\n",
    "> [nltk_data] Downloading package wordnet to\n",
    "> [nltk_data]   Package wordnet is already up-to-date!\n",
    "> [nltk_data] Downloading package averaged_perceptron_tagger to\n",
    "> [nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
    "> [nltk_data] Downloading package conll2000 to\n",
    "> [nltk_data]   Package conll2000 is already up-to-date!\n",
    "> [nltk_data] Downloading package movie_reviews to\n",
    "> [nltk_data]   Package movie_reviews is already up-to-date!\n",
    "> Finished.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# textblob 헬로월드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 감성점수 0.06000000000000001 : \n",
      "The titular threat of The Blob has always struck me as the ultimate movie\n",
      "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
      "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
      "describes it--\"assimilating flesh on contact.\n",
      "- 감성점수 -0.34166666666666673 : Snide comparisons to gelatin be damned, it's a concept with the most\n",
      "devastating of potential consequences, not unlike the grey goo scenario\n",
      "proposed by technological theorists fearful of\n",
      "artificial intelligence run rampant.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "\n",
    "text = '''\n",
    "The titular threat of The Blob has always struck me as the ultimate movie\n",
    "monster: an insatiably hungry, amoeba-like mass able to penetrate\n",
    "virtually any safeguard, capable of--as a doomed doctor chillingly\n",
    "describes it--\"assimilating flesh on contact.\n",
    "Snide comparisons to gelatin be damned, it's a concept with the most\n",
    "devastating of potential consequences, not unlike the grey goo scenario\n",
    "proposed by technological theorists fearful of\n",
    "artificial intelligence run rampant.\n",
    "'''\n",
    "\n",
    "blob = TextBlob(text)\n",
    "\n",
    "for sentence in blob.sentences:\n",
    "    print(f\"- 감성점수 {sentence.sentiment.polarity} : {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['The', 'titular', 'threat', 'of', 'The', 'Blob', 'has', 'always', 'struck', 'me', 'as', 'the', 'ultimate', 'movie', 'monster', 'an', 'insatiably', 'hungry', 'amoeba-like', 'mass', 'able', 'to', 'penetrate', 'virtually', 'any', 'safeguard', 'capable', 'of', 'as', 'a', 'doomed', 'doctor', 'chillingly', 'describes', 'it', 'assimilating', 'flesh', 'on', 'contact', 'Snide', 'comparisons', 'to', 'gelatin', 'be', 'damned', 'it', \"'s\", 'a', 'concept', 'with', 'the', 'most', 'devastating', 'of', 'potential', 'consequences', 'not', 'unlike', 'the', 'grey', 'goo', 'scenario', 'proposed', 'by', 'technological', 'theorists', 'fearful', 'of', 'artificial', 'intelligence', 'run', 'rampant'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DT</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IN</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>JJ</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NN</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNP</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNS</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRP</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RB</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RBS</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TO</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VB</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBG</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBN</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBZ</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     word\n",
       "pos      \n",
       "DT      9\n",
       "IN     10\n",
       "JJ     12\n",
       "NN     16\n",
       "NNP     1\n",
       "NNS     3\n",
       "PRP     3\n",
       "RB      5\n",
       "RBS     1\n",
       "TO      2\n",
       "VB      3\n",
       "VBG     1\n",
       "VBN     3\n",
       "VBZ     3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.tags[:5]\n",
    "# [('The', 'DT'),\n",
    "#  ('titular', 'JJ'),\n",
    "#  ('threat', 'NN'),\n",
    "#  ('of', 'IN'),\n",
    "#  ('The', 'DT')]\n",
    "text_df = pd.DataFrame(blob.tags, columns=['word', 'pos'])\n",
    "\n",
    "text_df.groupby('pos').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['titular threat', 'blob', 'ultimate movie monster', 'amoeba-like mass', 'snide', 'potential consequences', 'grey goo scenario', 'technological theorists fearful', 'artificial intelligence run rampant'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.noun_phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordNet 사전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 단어 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enjoying or showing or marked by joy or pleasure',\n",
       " 'marked by good fortune',\n",
       " 'eagerly disposed to act or to be of service',\n",
       " 'well expressed and to the point']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "from textblob.wordnet import VERB\n",
    "word = Word(\"happy\")\n",
    "word.definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동의어(synonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('happy.a.01'),\n",
       " Synset('felicitous.s.02'),\n",
       " Synset('glad.s.02'),\n",
       " Synset('happy.s.04')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word.synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'felicitous', 'glad', 'happy', 'well-chosen'}\n"
     ]
    }
   ],
   "source": [
    "synonyms = set()\n",
    "for synset in word.synsets:\n",
    "    for lemma in synset.lemmas():\n",
    "        synonyms.add(lemma.name())\n",
    "        \n",
    "print(synonyms)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 반의어(antonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('happy.a.01.happy')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas = word.synsets[0].lemmas()\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('unhappy.a.01.unhappy')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[0].antonyms()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
